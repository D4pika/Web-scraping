{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urltools\n",
    "import time \n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    from urllib.parse import urlsplit\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    \n",
    "\n",
    "    if base_url.startswith(\"https\"):\n",
    "        base_url = base_url.replace('https://','')\n",
    "    elif base_url.startswith(\"http\"):\n",
    "        base_url = base_url.replace('http://','')\n",
    "        \n",
    "    if base_url.endswith('/'):\n",
    "        base_url = base_url.replace('/','')\n",
    "    return base_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_page_exist(url):\n",
    "    exists = 0 \n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        if request.status_code == 200:\n",
    "            exists = 1 \n",
    "            \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract links from a URL------------------------------------------\n",
    "def href_scrapper(url):\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        a_tag = soup.find_all('a')\n",
    "        img_tag = soup.find_all('img')\n",
    "        link_tag = soup.find_all('link')\n",
    "        link_list = list()\n",
    "\n",
    "        for links in a_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in link_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in img_tag:\n",
    "            link_list.append(links.get('src'))\n",
    "\n",
    "        return link_list\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create complete links---------------------------------------------\n",
    "def link_former(url):\n",
    "    x = url\n",
    "\n",
    "    if x.startswith(\"http\"):\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        full_url = original_url + x\n",
    "        return full_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save links to directory of python file----------------------------\n",
    "def file_save(url, i):\n",
    "    save_error = 0\n",
    "    full_url = url\n",
    "    try:\n",
    "        if str(full_url).endswith(\".htm\"):\n",
    "            name = \"WebPage\" + str(i) + \".htm\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".pdf\"):\n",
    "            name = \"Pdf\" + str(i) + \".pdf\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpg\"):\n",
    "            name = \"Image\" + str(i) + \".jpg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpeg\"):\n",
    "            name = \"Image\" + str(i) + \".jpeg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".png\"):\n",
    "            name = \"Image\" + str(i) + \".png\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".css\"):\n",
    "            name = \"CSS File\" + str(i) + \".css\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        else:\n",
    "            name = \"WebPage\" + str(i) + \".html\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        return m\n",
    "    except:\n",
    "        save_error = save_error + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "def crawl(original_url, num_Id, output_file):\n",
    "    \n",
    "    # Accepting input of URL and depth----------------------------------------------\n",
    "    tic = time.time()\n",
    "    \n",
    "    parent_list = [original_url]\n",
    "    url_to_check = get_base_url(original_url)\n",
    "    print(url_to_check)\n",
    "    url_to_check = str(url_to_check)\n",
    "\n",
    "    layer_stop = 1\n",
    "    layer = 0\n",
    "\n",
    "\n",
    "    #initializing all the requiered lists---------------------------------------------\n",
    "    visited_all  = [original_url]\n",
    "    visited_current_layer = []\n",
    "    child_list =[]\n",
    "    child_list_filtered = [original_url]\n",
    "    #columns = ['Link','Parent Link', 'Layer']\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    Di={}\n",
    "\n",
    "    # Main execution of scrapper----------------------------------------------------\n",
    "\n",
    "    #looping through layers \n",
    "    while layer < layer_stop:\n",
    "    \n",
    "        #looping through URLs in parent-list\n",
    "        for url in parent_list:\n",
    "        \n",
    "            #scraping the children from the parent url----------------------------\n",
    "            if href_scrapper(url) != 0:\n",
    "                child_list = href_scrapper(url)\n",
    "    \n",
    "            \n",
    "            for child in child_list:\n",
    "                if child != None:\n",
    "                    ch=child\n",
    "                    #if child link is of the form \"index.php/blahblah\" and parent ends with '/'\n",
    "                    #---> \"parentlink/index.php/blahblah\"\n",
    "                    if child.startswith('/'):\n",
    "                        child= str(url) + str(child)\n",
    "                    \n",
    "                    if url.endswith('/') and url_to_check not in child:\n",
    "                        child = str(url) + str(child)\n",
    "                    \n",
    "                #normalize the child links-------------------------------------\n",
    "                    child=urltools.normalize(child)  \n",
    "                    \n",
    "\n",
    "                    #filtering out based on 1) External 2) Repeating 3) Invalid links---------------------------\n",
    "                    if url_to_check in child and child not in visited_all and does_page_exist(child)==1 and ch not in Di:\n",
    "                        child_list_filtered.append(child)\n",
    "                        Di[ch]=1\n",
    "                \n",
    "                    #adding everthing to visited all--------------------\n",
    "                    if child not in visited_all:\n",
    "                        child_slash = child + '/'\n",
    "                        visited_all.append(child)  \n",
    "                        visited_all.append(child_slash)\n",
    "                    \n",
    "                    \n",
    "            #adding  the visited and filtered children into the \"current visited layer\" ------ \n",
    "            for child_filtered in child_list_filtered:\n",
    "                visited_current_layer.append(child_filtered)\n",
    "\n",
    "            #creating a Pandas dataframe to store everything for download----------\n",
    "            layer_number = [layer+1]*len(child_list_filtered)\n",
    "            parent_of_child = [url]*len(child_list_filtered)\n",
    "\n",
    "            df_child = pd.DataFrame(child_list_filtered)\n",
    "            df_parent = pd.DataFrame(parent_of_child)\n",
    "            df_layer = pd.DataFrame(layer_number)\n",
    "\n",
    "\n",
    "            df_to_be_added = pd.concat([df_child,df_parent,df_layer], axis=1)\n",
    "            df = pd.concat([df,df_to_be_added],ignore_index=True, axis = 0)\n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "            #emptying the child lists\n",
    "            child_list = []\n",
    "            child_list_filtered = []\n",
    "        \n",
    "        #condition to stop filtering-----------------------------------------------\n",
    "        if not visited_current_layer :\n",
    "            layer_stop = layer_stop \n",
    "        else:\n",
    "            layer_stop += 1\n",
    "\n",
    "    \n",
    "        #child layer is now parent layer--------------------------------------------\n",
    "        parent_list = []\n",
    "    \n",
    "        #we only dont add .png, .jpg , .pdf to the new parent layer \n",
    "        for visited_current in visited_current_layer: \n",
    "            print(visited_current)\n",
    "            if(not visited_current.endswith('.png') and not visited_current.endswith('.jpg') and not  visited_current.endswith('.pdf')):\n",
    "                parent_list.append(visited_current)\n",
    "            \n",
    "            \n",
    "        #displaying the links in different layers----------------------------------\n",
    "        #print(\"Links in LAYER:\" + str(layer+1))\n",
    "        print(\"No of links = \" + str(len(visited_current_layer)))\n",
    "        #print(visited_current_layer)\n",
    "        print(\"\\n\")\n",
    "        visited_current_layer = [] \n",
    "        #updating the layer number\n",
    "        layer +=1\n",
    "    df.to_csv(output_file + '/' + str(num_Id) + '_' + str(url_to_check) +  '.csv', sep=',', encoding='utf-8')\n",
    "    return df, num_Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"https://docs.python.org/3/howto/urllib2.html\")\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "#information based on:\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "p_tag=soup.find_all('p')\n",
    "\n",
    "#extract text from html: \n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "web_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import os \n",
    "extensions=('.png', '.jpg','.jpeg','.pdf', '.gif','.ico')\n",
    "def scrape(df, scraped_website_path, website_index, website_name):\n",
    "    TempD={}\n",
    "    for index,item in df.iterrows():\n",
    "        print(item[1],item[0])\n",
    "        try:      \n",
    "            #for i in extensions:\n",
    "            #print(item[1])\n",
    "            if item[1].endswith(extensions):\n",
    "                print(item[1],\"skipped\")\n",
    "                pass\n",
    "            elif item[1].find(\".gif&\"):\n",
    "                print(item[1],\"skipped\")\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                print(item[1],\"printed\")\n",
    "                html = urllib.request.urlopen(item[1]).read()\n",
    "                #web_text.append(text_from_html(html))\n",
    "                print(text_from_html(html))\n",
    "                time.sleep(3)\n",
    "                #save_url(str(item[1]), index)\n",
    "                TempD[str(item[1])]=text_from_html(html)\n",
    "        #label: end\n",
    "        except: \n",
    "            pass\n",
    "    with open(scraped_website_path + '/' + 'json_' + str(website_index) + str(get_base_url(website_name)) +  '.json', 'w') as outfile:  \n",
    "        json.dump(TempD, outfile)\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# web_crawler_all() = Scraper for multiple websites# web_cr \n",
    "'''\n",
    "Inputs\n",
    "-----------\n",
    "excel_path = Excel file path which contains list of websites to be crawled  \n",
    "sheet = sheet name of this excel file \n",
    "web_col = the column no of in which the website hompages are located \n",
    "index_col = the column with indices of websites\n",
    "parent_col = the column with the parent names\n",
    "output_folder = path of the output folder where we wish to save the websites \n",
    "from_website = the index no. from website (typically starting from 1)\n",
    "to_website = the index no of the website till we want to continue scraping (scraped websites will include the \n",
    "                websites with this index)\n",
    "\n",
    "Output \n",
    "-----------\n",
    "CSV files containing a list of child links for each website are saved into the output_folder path \n",
    "\n",
    "'''\n",
    "def web_crawler_all(excel_path, sheet, web_col, index_col, parent_col,  output_folder, from_website, to_website):\n",
    "    df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "    df = df.iloc[from_website-1 : to_website, :]\n",
    "    print(df)\n",
    "    for index,row in df.iterrows():\n",
    "        num_Id=int(row[index_col])\n",
    "        original_url=row[web_col]\n",
    "        print(\"Website no : \" + str(num_Id))\n",
    "        print(\"Website name : \" + str(original_url))\n",
    "        \n",
    "        try:\n",
    "            websites= pd.read_csv(output_folder + str(int(num_Id)) + '_' + str(original_url) + '.csv')\n",
    "            print(\"website is already crawled\")\n",
    "        except:\n",
    "            websites, num_Id = crawl(original_url ,num_Id ,output_folder)\n",
    "            \n",
    "        \n",
    "    print(\"All links from the websites indicated have been crawled\")\n",
    "    print(\"Output File :  \" + str(output_folder))\n",
    "    print(\"No. of websites scraped : \" + str(to_website - from_website + 1))\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to be added\n",
    "- index to track what has been scrolled \n",
    "- time taken by the href function\n",
    "- file structure for all webpages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs \n",
    "----------\n",
    "excel_path = Excel file path which contains list of websites to be crawled  \n",
    "sheet = sheet name of this excel file \n",
    "crawled_website_path = path of file where all the crawled websites are stored  \n",
    "scraped_website_path = path of file where all the scraped websites are stored\n",
    "from_website - to_website = website index number from-to which we want to scrape and store in JSON \n",
    "web_col = column number of websites (0,1,2....)\n",
    "\n",
    "Output\n",
    "----------\n",
    "JSON files for each parent website in scraped_website_path folder. The path of the scraped websites \n",
    "is the same as that crawled websites \n",
    "'''\n",
    "\n",
    "def web_scraper_all(df, crawled_website_path, scraped_website_path, from_website, to_website,web_col):\n",
    "    #get excel sheet containing websites \n",
    "  #  df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "    df = df.iloc[from_website-1 : to_website, :]   \n",
    "    \n",
    "    #loop over all the websites and extract child links. Store them in JSON files \n",
    "    \n",
    "    for index, rows in df.iterrows():\n",
    "        #setting the website index and name \n",
    "        website_index = int(rows[0] )\n",
    "        website_name = rows[web_col]\n",
    "        \n",
    "        #generating filename and getting the csv file of child links \n",
    "        filename = crawled_website_path + '/'+ str(website_index)+ '_' + str(get_base_url(website_name)) + \".csv\"\n",
    "        df_of_child_links = pd.read_csv(filename)\n",
    "        print(filename,df_of_child_links )\n",
    "        #executing the scrape function \n",
    "        flag = scrape(df_of_child_links , scraped_website_path, website_index, website_name) \n",
    "        \n",
    "        #checkpoint to see if scraping is complete \n",
    "        if flag == 1 :\n",
    "            print(website_name + ' has been scraped successfully')\n",
    "            #print('scraping percentage complete = ' + str((i/to_website)*100) + '%')\n",
    "            flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executing the scraper function \n",
    "\n",
    "excel_path = 'D:\\WHIN/trial1.xlsx'\n",
    "sheet = 'From List of Companies'\n",
    "df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "crawled_website_path = 'D:\\WHIN\\Scraped_data/crawled_websites/'\n",
    "scraped_website_path = 'D:\\WHIN\\Scraped_data\\scraped_websites/'\n",
    "from_website = 4\n",
    "to_website = 5\n",
    "web_col = 2 \n",
    "df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "#print(df)\n",
    "web_scraper_all(df, crawled_website_path, scraped_website_path, from_website, to_website,web_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Serial No.               Company Name  \\\n",
      "3         5.0         Oerlikon Fairfield   \n",
      "4         6.0           Grand Industrial   \n",
      "5         7.0    Harrison Steel Castings   \n",
      "6         8.0            Hose Technology   \n",
      "7         9.0  Kirby Risk Service Center   \n",
      "8        10.0    Lafayette Wire Products   \n",
      "\n",
      "                                  Website  Has a Website?\\n(1=Yes, 0=No)  \\\n",
      "3  https://www.oerlikon.com/fairfield/en/                            NaN   \n",
      "4        https://www.grandindustrial.com/                            NaN   \n",
      "5                  http://www.hscast.com/                            NaN   \n",
      "6                     http://hosetec.com/                            NaN   \n",
      "7               https://www.kirbyrisk.com                            NaN   \n",
      "8               http://lafayettewire.com/                            NaN   \n",
      "\n",
      "  Static/Dynamic Website?  \\\n",
      "3                     NaN   \n",
      "4                     NaN   \n",
      "5                     NaN   \n",
      "6                     NaN   \n",
      "7                     NaN   \n",
      "8                     NaN   \n",
      "\n",
      "   If no website- Is present on Facebook?\\n(1=Yes, 0=No)  \n",
      "3                                                NaN      \n",
      "4                                                NaN      \n",
      "5                                                NaN      \n",
      "6                                                NaN      \n",
      "7                                                NaN      \n",
      "8                                                NaN      \n",
      "Website no : 5\n",
      "Website name : https://www.oerlikon.com/fairfield/en/\n",
      "www.oerlikon.com\n"
     ]
    }
   ],
   "source": [
    "#executing the crawler function \n",
    "\n",
    "excel_path = 'D:\\WHIN/trial1.xlsx'\n",
    "sheet = 'From List of Companies'\n",
    "web_col = 2\n",
    "index_col = 0\n",
    "parent_col = 1\n",
    "output_folder = 'D:\\WHIN\\scraped_data\\crawled_websites/'\n",
    "from_website = 4 \n",
    "to_website = 9\n",
    "\n",
    "web_crawler_all(excel_path,sheet, web_col, index_col, parent_col,output_folder, from_website, to_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape1(website_name):\n",
    "    TempD={}\n",
    "    #for index,item in df.iterrows():\n",
    "    extensions=('.png', '.jpg','.jpeg','.pdf', '.gif','.ico')    #file_save(item[1],item[0])\n",
    "    try:      \n",
    "        print(website_name)\n",
    "        print( website_name.endswith(\"extensions\"))\n",
    "        if 1:\n",
    "              # print(item[1])\n",
    "            html = urllib.request.urlopen(website_name).read()\n",
    "            #print(html)\n",
    "             #web_text.append(text_from_html(html))\n",
    "            print(text_from_html(html))\n",
    "            #    time.sleep(3)\n",
    "                #save_url(str(item[1]), index)\n",
    "           # TempD[str(item[1])]=text_from_html(html)\n",
    "    except: \n",
    "        pass\n",
    "   # with open(scraped_website_path + '/' + 'json_' + str(website_index) + str(get_base_url(website_name)) +  '.json', 'w') as outfile:  \n",
    "       # json.dump(TempD, outfile)\n",
    "        \n",
    "    return 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
