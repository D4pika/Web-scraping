{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urltools\n",
    "import time \n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    from urllib.parse import urlsplit\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    \n",
    "\n",
    "    if base_url.startswith(\"https\"):\n",
    "        base_url = base_url.replace('https://','')\n",
    "    elif base_url.startswith(\"http\"):\n",
    "        base_url = base_url.replace('http://','')\n",
    "        \n",
    "    if base_url.endswith('/'):\n",
    "        base_url = base_url.replace('/','')\n",
    "    return base_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_page_exist(url):\n",
    "    exists = 0 \n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        if request.status_code == 200:\n",
    "            exists = 1 \n",
    "            \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract links from a URL------------------------------------------\n",
    "def href_scrapper(url):\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        a_tag = soup.find_all('a')\n",
    "        img_tag = soup.find_all('img')\n",
    "        link_tag = soup.find_all('link')\n",
    "        link_list = list()\n",
    "\n",
    "        for links in a_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in link_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in img_tag:\n",
    "            link_list.append(links.get('src'))\n",
    "\n",
    "        return link_list\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create complete links---------------------------------------------\n",
    "def link_former(url):\n",
    "    x = url\n",
    "\n",
    "    if x.startswith(\"http\"):\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        full_url = original_url + x\n",
    "        return full_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save links to directory of python file----------------------------\n",
    "def file_save(url, i):\n",
    "    save_error = 0\n",
    "    full_url = url\n",
    "    try:\n",
    "        if str(full_url).endswith(\".htm\"):\n",
    "            name = \"WebPage\" + str(i) + \".htm\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".pdf\"):\n",
    "            name = \"Pdf\" + str(i) + \".pdf\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpg\"):\n",
    "            name = \"Image\" + str(i) + \".jpg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpeg\"):\n",
    "            name = \"Image\" + str(i) + \".jpeg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".png\"):\n",
    "            name = \"Image\" + str(i) + \".png\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".css\"):\n",
    "            name = \"CSS File\" + str(i) + \".css\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        else:\n",
    "            name = \"WebPage\" + str(i) + \".html\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        return m\n",
    "    except:\n",
    "        save_error = save_error + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "def crawl(original_url, num_Id, output_file):\n",
    "    \n",
    "    # Accepting input of URL and depth----------------------------------------------\n",
    "    tic = time.time()\n",
    "    \n",
    "    parent_list = [original_url]\n",
    "    url_to_check = get_base_url(original_url)\n",
    "    print(url_to_check)\n",
    "    url_to_check = str(url_to_check)\n",
    "\n",
    "    layer_stop = 1\n",
    "    layer = 0\n",
    "\n",
    "\n",
    "    #initializing all the requiered lists---------------------------------------------\n",
    "    visited_all  = [original_url]\n",
    "    visited_current_layer = []\n",
    "    child_list =[]\n",
    "    child_list_filtered = []\n",
    "    #columns = ['Link','Parent Link', 'Layer']\n",
    "    df = pd.DataFrame()\n",
    "    Di={}\n",
    "\n",
    "    # Main execution of scrapper----------------------------------------------------\n",
    "\n",
    "    #looping through layers \n",
    "    while layer < layer_stop:\n",
    "    \n",
    "        #looping through URLs in parent-list\n",
    "        for url in parent_list:\n",
    "        \n",
    "            #scraping the children from the parent url----------------------------\n",
    "            if href_scrapper(url) != 0:\n",
    "                child_list = href_scrapper(url)\n",
    "    \n",
    "            \n",
    "            for child in child_list:\n",
    "                if child != None:\n",
    "                    ch=child\n",
    "                    #if child link is of the form \"index.php/blahblah\" and parent ends with '/'\n",
    "                    #---> \"parentlink/index.php/blahblah\"\n",
    "                    if child.startswith('/'):\n",
    "                        child= str(url) + str(child)\n",
    "                    \n",
    "                    if url.endswith('/') and url_to_check not in child and ch not in Di:\n",
    "                        child = str(url) + str(child)\n",
    "                    \n",
    "                #normalize the child links-------------------------------------\n",
    "                    child=urltools.normalize(child)  \n",
    "                    \n",
    "\n",
    "                    #filtering out based on 1) External 2) Repeating 3) Invalid links---------------------------\n",
    "                    if url_to_check in child and child not in visited_all and does_page_exist(child)==1 and ch not in Di:\n",
    "                        child_list_filtered.append(child)\n",
    "                        Di[ch]=1\n",
    "                \n",
    "                    #adding everthing to visited all--------------------\n",
    "                    if child not in visited_all:\n",
    "                        child_slash = child + '/'\n",
    "                        visited_all.append(child)  \n",
    "                        visited_all.append(child_slash)\n",
    "                    \n",
    "                    \n",
    "            #adding  the visited and filtered children into the \"current visited layer\" ------ \n",
    "            for child_filtered in child_list_filtered:\n",
    "                visited_current_layer.append(child_filtered)\n",
    "\n",
    "            #creating a Pandas dataframe to store everything for download----------\n",
    "            layer_number = [layer+1]*len(child_list_filtered)\n",
    "            parent_of_child = [url]*len(child_list_filtered)\n",
    "\n",
    "            df_child = pd.DataFrame(child_list_filtered)\n",
    "            df_parent = pd.DataFrame(parent_of_child)\n",
    "            df_layer = pd.DataFrame(layer_number)\n",
    "\n",
    "\n",
    "            df_to_be_added = pd.concat([df_child,df_parent,df_layer], axis=1)\n",
    "            df = pd.concat([df,df_to_be_added],ignore_index=True, axis = 0)\n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "            #emptying the child lists\n",
    "            child_list = []\n",
    "            child_list_filtered = []\n",
    "        \n",
    "        #condition to stop filtering-----------------------------------------------\n",
    "        if not visited_current_layer :\n",
    "            layer_stop = layer_stop \n",
    "        else:\n",
    "            layer_stop += 1\n",
    "\n",
    "    \n",
    "        #child layer is now parent layer--------------------------------------------\n",
    "        parent_list = []\n",
    "    \n",
    "        #we only dont add .png, .jpg , .pdf to the new parent layer \n",
    "        for visited_current in visited_current_layer: \n",
    "            print(visited_current)\n",
    "            if(not visited_current.endswith('.png') and not visited_current.endswith('.jpg') and not  visited_current.endswith('.pdf')):\n",
    "                parent_list.append(visited_current)\n",
    "            \n",
    "            \n",
    "        #displaying the links in different layers----------------------------------\n",
    "        #print(\"Links in LAYER:\" + str(layer+1))\n",
    "        print(\"No of links = \" + str(len(visited_current_layer)))\n",
    "        #print(visited_current_layer)\n",
    "        print(\"\\n\")\n",
    "        visited_current_layer = [] \n",
    "        #updating the layer number\n",
    "        layer +=1\n",
    "    df.to_csv(output_file + '/' + str(num_Id) + '_' + str(url_to_check) +  '.csv', sep=',', encoding='utf-8')\n",
    "    return df, num_Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"https://docs.python.org/3/howto/urllib2.html\")\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "#information based on:\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "p_tag=soup.find_all('p')\n",
    "\n",
    "#extract text from html: \n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "web_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import os \n",
    "extensions=['.png', '.jpg','jpeg','.pdf', '.css', '.gif','.ico']\n",
    "def scrape(df, scraped_website_path, website_index, website_name):\n",
    "    TempD={}\n",
    "    for index,item in df.iterrows():\n",
    "        file_save(item[1],item[0])\n",
    "        try:      \n",
    "            print(item[1])\n",
    "            if item.endswith(extensions):\n",
    "                pass\n",
    "            html = urllib.request.urlopen(str(item[1])).read()\n",
    "            #web_text.append(text_from_html(html))\n",
    "            print(text_from_html(html))\n",
    "            time.sleep(3)\n",
    "            #save_url(str(item[1]), index)\n",
    "            TempD[str(item[1])]=text_from_html(html)\n",
    "        except: \n",
    "            pass\n",
    "    with open(scraped_website_path + '/' + 'json_' + str(website_index) + str(get_base_url(website_name)) +  '.json', 'w') as outfile:  \n",
    "        json.dump(TempD, outfile)\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# web_crawler_all() = Scraper for multiple websites# web_cr \n",
    "'''\n",
    "Inputs\n",
    "-----------\n",
    "excel_path = Excel file path which contains list of websites to be crawled  \n",
    "sheet = sheet name of this excel file \n",
    "web_col = the column no of in which the website hompages are located \n",
    "index_col = the column with indices of websites\n",
    "parent_col = the column with the parent names\n",
    "output_folder = path of the output folder where we wish to save the websites \n",
    "from_website = the index no. from website (typically starting from 1)\n",
    "to_website = the index no of the website till we want to continue scraping (scraped websites will include the \n",
    "                websites with this index)\n",
    "\n",
    "Output \n",
    "-----------\n",
    "CSV files containing a list of child links for each website are saved into the output_folder path \n",
    "\n",
    "'''\n",
    "def web_crawler_all(excel_path, sheet, web_col, index_col, parent_col,  output_folder, from_website, to_website):\n",
    "    df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "    df = df.iloc[from_website-1 : to_website, :]\n",
    "    print(df)\n",
    "    for index,row in df.iterrows():\n",
    "        num_Id=int(row[index_col])\n",
    "        original_url=row[web_col]\n",
    "        print(\"Website no : \" + str(num_Id))\n",
    "        print(\"Website name : \" + str(original_url))\n",
    "        \n",
    "        try:\n",
    "            websites= pd.read_csv(output_folder + str(int(num_Id)) + '_' + str(original_url) + '.csv')\n",
    "            print(\"website is already crawled\")\n",
    "        except:\n",
    "            websites, num_Id = crawl(original_url ,num_Id ,output_folder)\n",
    "            \n",
    "        \n",
    "    print(\"All links from the websites indicated have been crawled\")\n",
    "    print(\"Output File :  \" + str(output_folder))\n",
    "    print(\"No. of websites scraped : \" + str(to_website - from_website + 1))\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Serial No.          Company Name                          Website  \\\n",
      "0         2.0    Carmel Engineering        http://www.carmeleng.com/   \n",
      "1         3.0  Dayton-Phoenix Group  https://www.dayton-phoenix.com/   \n",
      "\n",
      "   Has a Website?\\n(1=Yes, 0=No) Static/Dynamic Website?  \\\n",
      "0                            1.0                       D   \n",
      "1                            1.0                       D   \n",
      "\n",
      "   If no website- Is present on Facebook?\\n(1=Yes, 0=No)  \n",
      "0                                                NaN      \n",
      "1                                                NaN      \n",
      "Website no : 2\n",
      "Website name : http://www.carmeleng.com/\n",
      "www.carmeleng.com\n",
      "http://www.carmeleng.com/contact_us.asp\n",
      "http://www.carmeleng.com/newsletter.asp\n",
      "http://www.carmeleng.com/http:/www.gobuckaroo.com\n",
      "http://www.carmeleng.com/default.asp\n",
      "http://www.carmeleng.com//favicon.ico\n",
      "http://www.carmeleng.com/carmelstyle.css\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_03.jpg\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_NEW_16.jpg\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_25.jpg\n",
      "http://www.carmeleng.com/images/white.gif\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_NEW_18.jpg\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_22.jpg\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_30.jpg\n",
      "http://www.carmeleng.com/images/CE_SplashPage-2_32.jpg\n",
      "No of links = 14\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of links = 0\n",
      "\n",
      "\n",
      "Website no : 3\n",
      "Website name : https://www.dayton-phoenix.com/\n",
      "www.dayton-phoenix.com\n",
      "https://www.dayton-phoenix.com/siteMap.php\n",
      "https://www.dayton-phoenix.com/siteCredits.php\n",
      "https://www.dayton-phoenix.com/divisions.php\n",
      "https://www.dayton-phoenix.com/news.php\n",
      "https://www.dayton-phoenix.com/productCategories.php\n",
      "https://www.dayton-phoenix.com/serviceCenters.php\n",
      "https://www.dayton-phoenix.com/contact.php\n",
      "https://www.dayton-phoenix.com//divisions.php\n",
      "https://www.dayton-phoenix.com/index.php?dt=FL\n",
      "https://www.dayton-phoenix.com/index.php?dt=PL\n",
      "https://www.dayton-phoenix.com/index.php?dt=OH\n",
      "https://www.dayton-phoenix.com/index.php?dt=ES\n",
      "https://www.dayton-phoenix.com/index.php?dt=SV\n",
      "https://www.dayton-phoenix.com/news.php?newsId=38\n",
      "https://www.dayton-phoenix.com/news.php?newsId=37\n",
      "https://www.dayton-phoenix.com/news.php?newsId=36\n",
      "https://www.dayton-phoenix.com/news.php?newsId=35\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=106\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=19\n",
      "https://www.dayton-phoenix.com/dpg.css\n",
      "https://www.dayton-phoenix.com/colorbox.css\n",
      "https://www.dayton-phoenix.com/rss.php\n",
      "https://www.dayton-phoenix.com/i/dpgLogo.gif\n",
      "https://www.dayton-phoenix.com/i/divFL.png\n",
      "https://www.dayton-phoenix.com/i/divButton-on.gif\n",
      "https://www.dayton-phoenix.com/i/divButton-off.gif\n",
      "https://www.dayton-phoenix.com/i/catThumb/RadiatorCoolingFan.png\n",
      "https://www.dayton-phoenix.com/i/catThumb/BellRinger.png\n",
      "No of links = 28\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=115\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=54\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=55\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=21\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=92\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=58\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=122\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=124\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=116\n",
      "https://www.dayton-phoenix.com/news.php?newsId=34\n",
      "https://www.dayton-phoenix.com/news.php?newsId=27\n",
      "https://www.dayton-phoenix.com/news.php?newsId=26\n",
      "https://www.dayton-phoenix.com/minerals.php\n",
      "https://www.dayton-phoenix.com/news.php?newsId=28\n",
      "https://www.dayton-phoenix.com/suppdiv.php\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=59\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=103\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=46\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=104\n",
      "https://www.dayton-phoenix.com/history.php\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=62\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=135\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=117\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=127\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=129\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=95\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=94\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=98\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=93\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=118\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=128\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=120\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=81\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=139\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=85\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=74\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=73\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=100\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=77\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=108\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=72\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=50\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=45\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=15\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=16\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=53\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=18\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=40\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=111\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=107\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=110\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=109\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=80\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=2\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=105\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=97\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=131\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=4\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=11\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=8\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=44\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=31\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=79\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=52\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=134\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=101\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=78\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=114\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=99\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=138\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=30\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=32\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=14\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=112\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=25\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=137\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=76\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=75\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=10\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=22\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=48\n",
      "https://www.dayton-phoenix.com/productCategoryDetail.php?categoryId=24\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=102\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=71\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=64\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=60\n",
      "https://www.dayton-phoenix.com/news.php?newsId=33\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=123\n",
      "https://www.dayton-phoenix.com/news.php?newsId=32\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=33\n",
      "https://www.dayton-phoenix.com/facilities.php\n",
      "https://www.dayton-phoenix.com/certs_dayton.php\n",
      "https://www.dayton-phoenix.com/certs_goth.php\n",
      "https://www.dayton-phoenix.com/certs_wl.php\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=7\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=96\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=39\n",
      "https://www.dayton-phoenix.com/sales.php\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/\n",
      "https://www.dayton-phoenix.com/productCategories.php?categoryId=26\n",
      "https://www.dayton-phoenix.com/warranty.php\n",
      "https://www.dayton-phoenix.com/documents/DPG%20Terms%20&%20Conditions%20004-03-17.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/DPG%20Announces%20new%20ADVANTAGE%20SERVICES%20for%20GP%2038-40%20and%20SD40%20Locomotives%20.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/DPG%20Announces%20new%20SERIES%20Rooftop%20AC%20Phase%20out%20of%20older%20unit%20May%204%20%202018%20R%20.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Dayton-Phoenix%20Group%20Announces%20new%20Advantage%20SeriesTM%20Cab%20Heater%20and%20Phase%20Out%20of%20Older%20PM%20Series%20Cab%20Heater.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Announces%20Advantage%20Series%2042517.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Announces%20Attendance%20at%20ASLRRA%20Connections%20April%20%202018.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Announces%20Attendance%20at%20Railwa%202017%20Interchange%20Conference.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Announces%20Nick%20Frazier.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Grid%20Resistor%20Quality%20.pdf\n",
      "https://www.dayton-phoenix.com/documents/uploads/Press%20Release%20DPG%20Receives%20TPG%20Accrediation%20.pdf\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/AR&R.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/ARRINSTR.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/CONTROL.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/CPINST.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/SAMPINSP.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/SURVEY.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/VR&RINST.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/WARRANT.DOC\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/WARRINST.DOC\n",
      "https://www.dayton-phoenix.com/news.php/documents/uploads/DPG Announces new ADVANTAGE SERVICES for GP 38-40 and SD40 Locomotives .pdf\n",
      "https://www.dayton-phoenix.com/productCategories.php/documents/DPG%20Terms%20&%20Conditions%20004-03-17.pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=37/documents/uploads/DPG Announces new SERIES Rooftop AC Phase out of older unit May 4  2018 R .pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=36/documents/uploads/Press Release DPG Announces Attendance at ASLRRA Connections April  2018.pdf\n",
      "https://www.dayton-phoenix.com/rss.php/documents/uploads/Press Release DPG Announces Attendance at Railwa 2017 Interchange Conference.pdf\n",
      "No of links = 125\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dayton-phoenix.com/news.php?newsId=27/documents/uploads/Press Release DPG Grid Resistor Quality .pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=26/documents/uploads/Press Release DPG Announces Nick Frazier.pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=28/documents/uploads/Press Release DPG Receives TPG Accrediation .pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=33/documents/uploads/Dayton-Phoenix Group Announces new Advantage SeriesTM Cab Heater and Phase Out of Older PM Series Cab Heater.pdf\n",
      "https://www.dayton-phoenix.com/news.php?newsId=32/documents/uploads/Press Release DPG Announces Advantage Series 42517.pdf\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_General Description\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Requirements\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Detailed StepsOverview of Steps\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Detailed Steps\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Revision History\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Authorization History\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Contributing Authors\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Menu\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_1\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_2\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_9\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_13\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_17\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_22\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_27\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/#Nav_Screenid_789_dsn_30\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/VR&R.XLS\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/west_lafayette_facility_color_small.jpg\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/z_navicon_top.gif\n",
      "https://www.dayton-phoenix.com/dynamic_supplier_manual/z_navicon_overview.gif\n",
      "No of links = 25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#executing the crawler function \n",
    "\n",
    "excel_path = 'D:\\WHIN/trial1.xlsx'\n",
    "sheet = 'From List of Companies'\n",
    "web_col = 2\n",
    "index_col = 0\n",
    "parent_col = 1\n",
    "output_folder = 'D:\\WHIN\\scraped_data\\crawled_websites/'\n",
    "from_website = 1 \n",
    "to_website = 2 \n",
    "\n",
    "web_crawler_all(excel_path,sheet, web_col, index_col, parent_col,output_folder, from_website, to_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
