{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urlTools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d06c033593d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murlTools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urlTools'"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urlTools\n",
    "import time \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "#location=\"`D:\\WHIN\\\"\n",
    "\n",
    "def get_base_url(url):\n",
    "    from urllib.parse import urlsplit\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    \n",
    "\n",
    "    if base_url.startswith(\"https\"):\n",
    "        base_url = base_url.replace('https://','')\n",
    "    elif base_url.startswith(\"http\"):\n",
    "        base_url = base_url.replace('http://','')\n",
    "        \n",
    "    if base_url.endswith('/'):\n",
    "        base_url = base_url.replace('/','')\n",
    "    return base_url\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def does_page_exist(url):\n",
    "    exists = 0 \n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        if request.status_code == 200:\n",
    "            exists = 1 \n",
    "            \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return exists\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# Function to extract links from a URL------------------------------------------\n",
    "def href_scrapper(url):\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        a_tag = soup.find_all('a')\n",
    "        img_tag = soup.find_all('img')\n",
    "        link_tag = soup.find_all('link')\n",
    "        link_list = list()\n",
    "\n",
    "        for links in a_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in link_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in img_tag:\n",
    "            link_list.append(links.get('src'))\n",
    "\n",
    "        return link_list\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Function to create complete links---------------------------------------------\n",
    "def link_former(url):\n",
    "    x = url\n",
    "\n",
    "    if x.startswith(\"http\"):\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        full_url = original_url + x\n",
    "        return full_url\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# Function to save links to directory of python file----------------------------\n",
    "def file_save(url, i):\n",
    "    save_error = 0\n",
    "    full_url = url\n",
    "    try:\n",
    "        if str(full_url).endswith(\".htm\"):\n",
    "            name = \"WebPage\" + str(i) + \".htm\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".pdf\"):\n",
    "            name = \"Pdf\" + str(i) + \".pdf\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpg\"):\n",
    "            name = \"Image\" + str(i) + \".jpg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpeg\"):\n",
    "            name = \"Image\" + str(i) + \".jpeg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".png\"):\n",
    "            name = \"Image\" + str(i) + \".png\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".css\"):\n",
    "            name = \"CSS File\" + str(i) + \".css\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        else:\n",
    "            name = \"WebPage\" + str(i) + \".html\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        return m\n",
    "    except:\n",
    "        save_error = save_error + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def crawl(original_url, num_Id):\n",
    "    \n",
    "    # Accepting input of URL and depth----------------------------------------------\n",
    "    tic = time.time()\n",
    "    \n",
    "    parent_list = [original_url]\n",
    "    url_to_check = get_base_url(original_url)\n",
    "    print(url_to_check)\n",
    "    url_to_check = str(url_to_check)\n",
    "\n",
    "    layer_stop = 1\n",
    "    layer = 0\n",
    "\n",
    "\n",
    "    #initializing all the requiered lists---------------------------------------------\n",
    "    visited_all  = [original_url]\n",
    "    visited_current_layer = []\n",
    "    child_list =[]\n",
    "    child_list_filtered = []\n",
    "    #columns = ['Link','Parent Link', 'Layer']\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Main execution of scrapper----------------------------------------------------\n",
    "\n",
    "    #looping through layers \n",
    "    while layer < layer_stop and layer <7:\n",
    "    \n",
    "        #looping through URLs in parent-list\n",
    "        for url in parent_list:\n",
    "        \n",
    "            #scraping the children from the parent url----------------------------\n",
    "            if href_scrapper(url) != 0:\n",
    "                child_list = href_scrapper(url)\n",
    "    \n",
    "    \n",
    "            for child in child_list:\n",
    "                if child != None:\n",
    "                    #if child link is of the form \"index.php/blahblah\" and parent ends with '/'\n",
    "                    #---> \"parentlink/index.php/blahblah\"\n",
    "                    if child.startswith('/'):\n",
    "                        child= str(url) + str(child)\n",
    "                    \n",
    "                    if url.endswith('/') and url_to_check not in child:\n",
    "                        child = str(url) + str(child)\n",
    "                    \n",
    "                #normalize the child links-------------------------------------\n",
    "                    child=urltools.normalize(child)  \n",
    "                    \n",
    "\n",
    "                    #filtering out based on 1) External 2) Repeating 3) Invalid links---------------------------\n",
    "                    if url_to_check in child and child not in visited_all and does_page_exist(child)==1:\n",
    "                        child_list_filtered.append(child)\n",
    "                \n",
    "                    #adding everthing to visited all--------------------\n",
    "                    if child not in visited_all:\n",
    "                        child_slash = child + '/'\n",
    "                        visited_all.append(child)  \n",
    "                        visited_all.append(child_slash)\n",
    "                    \n",
    "                    \n",
    "            #adding  the visited and filtered children into the \"current visited layer\" ------ \n",
    "            for child_filtered in child_list_filtered:\n",
    "                visited_current_layer.append(child_filtered)\n",
    "\n",
    "            #creating a Pandas dataframe to store everything for download----------\n",
    "            layer_number = [layer+1]*len(child_list_filtered)\n",
    "            parent_of_child = [url]*len(child_list_filtered)\n",
    "            for visited_current in visited_current_layer: \n",
    "                print(visited_current)\n",
    "            if(not visited_current.endswith('.png') and not visited_current.endswith('.jpg') and not  visited_current.endswith('.pdf')):\n",
    "                parent_list.append(visited_current)\n",
    "\n",
    "            df_child = pd.DataFrame(child_list_filtered)\n",
    "            df_parent = pd.DataFrame(parent_of_child)\n",
    "            df_layer = pd.DataFrame(layer_number)\n",
    "\n",
    "\n",
    "            df_to_be_added = pd.concat([df_child,df_parent,df_layer], axis=1)\n",
    "            df = pd.concat([df,df_to_be_added],ignore_index=True, axis = 0)\n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "            #emptying the child lists\n",
    "            child_list = []\n",
    "            child_list_filtered = []\n",
    "        \n",
    "        #condition to stop filtering-----------------------------------------------\n",
    "        if not visited_current_layer :\n",
    "            layer_stop = layer_stop \n",
    "        else:\n",
    "            layer_stop += 1\n",
    "\n",
    "    \n",
    "        #child layer is now parent layer--------------------------------------------\n",
    "        parent_list = []\n",
    "    \n",
    "        #we only dont add .png, .jpg , .pdf to the new parent layer \n",
    "        \n",
    "            \n",
    "            \n",
    "        #displaying the links in different layers----------------------------------\n",
    "        #print(\"Links in LAYER:\" + str(layer+1))\n",
    "        print(\"No of links = \" + str(len(visited_current_layer)))\n",
    "        #print(visited_current_layer)\n",
    "        print(\"\\n\")\n",
    "        visited_current_layer = [] \n",
    "        #updating the layer number\n",
    "        layer +=1\n",
    "    file_name = 'D:\\WHIN\\website'\n",
    "    df.to_csv(file_name + str(num_Id) + '.csv', sep=',', encoding='utf-8')\n",
    "    \n",
    "    return df, num_Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"https://docs.python.org/3/howto/urllib2.html\")\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "#information based on:\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "p_tag=soup.find_all('p')\n",
    "\n",
    "#extract text from html: \n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "web_text = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import os \n",
    "os.chdir(\"D:\\WHIN/\")\n",
    "def scrape(df, num_Id):\n",
    "    TempD={}\n",
    "    for index,item in df.iterrows():\n",
    "        try:      \n",
    "            print(item[1])\n",
    "            html = urllib.request.urlopen(str(item[1])).read()\n",
    "            web_text.append(text_from_html(html))\n",
    "            print(text_from_html(html))\n",
    "            save_url(str(item[1]), index)\n",
    "            TempD[item[1]]=text_from_html(html)\n",
    "        except: \n",
    "            pass\n",
    "    with open('json'+str(num_Id)+'.json', 'w') as outfile:  \n",
    "        json.dump(TempD, outfile)\n",
    "        \n",
    "    return 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Website</th>\n",
       "      <th>Has a Website?\n",
       "(1=Yes, 0=No)</th>\n",
       "      <th>Static/Dynamic Website?</th>\n",
       "      <th>If no website- Is present on Facebook?\n",
       "(1=Yes, 0=No)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Carmel Engineering</td>\n",
       "      <td>http://www.carmeleng.com/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Dayton-Phoenix Group</td>\n",
       "      <td>https://www.dayton-phoenix.com/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Dyna-Fab</td>\n",
       "      <td>http://dyna-fab.org/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Oerlikon Fairfield</td>\n",
       "      <td>https://www.oerlikon.com/fairfield/en/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Grand Industrial</td>\n",
       "      <td>grandindustrial.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Harrison Steel Castings</td>\n",
       "      <td>http://www.hscast.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Hose Technology</td>\n",
       "      <td>http://hosetec.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Kirby Risk Service Center</td>\n",
       "      <td>https://www.kirbyrisk.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Lafayette Wire Products</td>\n",
       "      <td>http://lafayettewire.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.0</td>\n",
       "      <td>Logan Stampings</td>\n",
       "      <td>http://loganstampings.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Serial No.               Company Name  \\\n",
       "0          2.0         Carmel Engineering   \n",
       "1          3.0       Dayton-Phoenix Group   \n",
       "2          4.0                   Dyna-Fab   \n",
       "3          5.0         Oerlikon Fairfield   \n",
       "4          6.0           Grand Industrial   \n",
       "5          7.0    Harrison Steel Castings   \n",
       "6          8.0            Hose Technology   \n",
       "7          9.0  Kirby Risk Service Center   \n",
       "8         10.0    Lafayette Wire Products   \n",
       "9         11.0            Logan Stampings   \n",
       "10         NaN                        NaN   \n",
       "11         NaN                        NaN   \n",
       "12         NaN                        NaN   \n",
       "13         NaN                        NaN   \n",
       "14         NaN                        NaN   \n",
       "15         NaN                        NaN   \n",
       "16         NaN                        NaN   \n",
       "17         NaN                        NaN   \n",
       "18         NaN                        NaN   \n",
       "19         NaN                        NaN   \n",
       "20         NaN                        NaN   \n",
       "21         NaN                        NaN   \n",
       "22         NaN                        NaN   \n",
       "23         NaN                        NaN   \n",
       "24         NaN                        NaN   \n",
       "25         NaN                        NaN   \n",
       "26         NaN                        NaN   \n",
       "27         NaN                        NaN   \n",
       "28         NaN                        NaN   \n",
       "29         NaN                        NaN   \n",
       "\n",
       "                                   Website  Has a Website?\\n(1=Yes, 0=No)  \\\n",
       "0                http://www.carmeleng.com/                            1.0   \n",
       "1          https://www.dayton-phoenix.com/                            1.0   \n",
       "2                     http://dyna-fab.org/                            NaN   \n",
       "3   https://www.oerlikon.com/fairfield/en/                            NaN   \n",
       "4                      grandindustrial.com                            NaN   \n",
       "5                   http://www.hscast.com/                            NaN   \n",
       "6                      http://hosetec.com/                            NaN   \n",
       "7                https://www.kirbyrisk.com                            NaN   \n",
       "8                http://lafayettewire.com/                            NaN   \n",
       "9               http://loganstampings.com/                            NaN   \n",
       "10                                     NaN                            NaN   \n",
       "11                                     NaN                            NaN   \n",
       "12                                     NaN                            NaN   \n",
       "13                                     NaN                            NaN   \n",
       "14                                     NaN                            NaN   \n",
       "15                                     NaN                            NaN   \n",
       "16                                     NaN                            NaN   \n",
       "17                                     NaN                            NaN   \n",
       "18                                     NaN                            NaN   \n",
       "19                                     NaN                            NaN   \n",
       "20                                     NaN                            NaN   \n",
       "21                                     NaN                            NaN   \n",
       "22                                     NaN                            NaN   \n",
       "23                                     NaN                            NaN   \n",
       "24                                     NaN                            3.4   \n",
       "25                                     NaN                            2.4   \n",
       "26                                     NaN                            1.7   \n",
       "27                                     NaN                            1.7   \n",
       "28                                     NaN                            1.8   \n",
       "29                                     NaN                            1.6   \n",
       "\n",
       "   Static/Dynamic Website?  \\\n",
       "0                        D   \n",
       "1                        D   \n",
       "2                      NaN   \n",
       "3                      NaN   \n",
       "4                      NaN   \n",
       "5                      NaN   \n",
       "6                      NaN   \n",
       "7                      NaN   \n",
       "8                      NaN   \n",
       "9                      NaN   \n",
       "10                     NaN   \n",
       "11                     NaN   \n",
       "12                     NaN   \n",
       "13                     NaN   \n",
       "14                     NaN   \n",
       "15                     NaN   \n",
       "16                     NaN   \n",
       "17                     NaN   \n",
       "18                     NaN   \n",
       "19                     NaN   \n",
       "20                     NaN   \n",
       "21                     NaN   \n",
       "22                     NaN   \n",
       "23                     NaN   \n",
       "24                     NaN   \n",
       "25                     NaN   \n",
       "26                     NaN   \n",
       "27                     NaN   \n",
       "28                     NaN   \n",
       "29                     NaN   \n",
       "\n",
       "    If no website- Is present on Facebook?\\n(1=Yes, 0=No)  \n",
       "0                                                 NaN      \n",
       "1                                                 NaN      \n",
       "2                                                 NaN      \n",
       "3                                                 NaN      \n",
       "4                                                 NaN      \n",
       "5                                                 NaN      \n",
       "6                                                 NaN      \n",
       "7                                                 NaN      \n",
       "8                                                 NaN      \n",
       "9                                                 NaN      \n",
       "10                                                NaN      \n",
       "11                                                NaN      \n",
       "12                                                NaN      \n",
       "13                                                NaN      \n",
       "14                                                NaN      \n",
       "15                                                NaN      \n",
       "16                                                NaN      \n",
       "17                                                NaN      \n",
       "18                                                NaN      \n",
       "19                                                NaN      \n",
       "20                                                NaN      \n",
       "21                                                NaN      \n",
       "22                                                NaN      \n",
       "23                                                NaN      \n",
       "24                                                NaN      \n",
       "25                                                NaN      \n",
       "26                                                NaN      \n",
       "27                                                NaN      \n",
       "28                                                NaN      \n",
       "29                                                NaN      "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "excel_file = 'D:\\WHIN/Trial1.xlsx'\n",
    "df = pd.read_excel(excel_file , sheet_name='From List of Companies')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.carmeleng.com\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'urltools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-96454ec2898f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mwebsites\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"website\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'website2.0.csv' does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-96454ec2898f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mwebsites\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"website\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mwebsites\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_Id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"website\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdf_of_child_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-7cd6a87f64e8>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(original_url, num_Id)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;31m#normalize the child links-------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[0mchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murltools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urltools' is not defined"
     ]
    }
   ],
   "source": [
    "#this is the code to run everything at once: \n",
    "i = 1 \n",
    "for index,row in df.iterrows():\n",
    "    num_Id=row[0]\n",
    "    original_url=row[2]\n",
    "    try:\n",
    "        websites= pd.read_csv(\"website\"+str(row[0])+ '.csv')\n",
    "    except:\n",
    "        websites, num_Id = crawl(row[2],row[0])\n",
    "        filename = \"website\" + str(int(row[0]) )+ \".csv\"\n",
    "        df_of_child_links = pd.read_csv(filename)\n",
    "        df_of_child_links\n",
    "        flag = scrape(df_of_child_links ,i) \n",
    "        \n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('json1.json') as f1:\n",
    "    data1 = json.load(f1)\n",
    "    \n",
    "with open('json2.json') as f2:\n",
    "    data2 = json.load(f2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
