{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urltools\n",
    "import time \n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "\n",
    "\n",
    "def get_base_url(url):\n",
    "    from urllib.parse import urlsplit\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    \n",
    "\n",
    "    if base_url.startswith(\"https\"):\n",
    "        base_url = base_url.replace('https://','')\n",
    "    elif base_url.startswith(\"http\"):\n",
    "        base_url = base_url.replace('http://','')\n",
    "        \n",
    "    if base_url.endswith('/'):\n",
    "        base_url = base_url.replace('/','')\n",
    "    return base_url\n",
    "\n",
    "\n",
    "\n",
    "def does_page_exist(url):\n",
    "    exists = 0 \n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        if request.status_code == 200:\n",
    "            exists = 1 \n",
    "            \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return exists\n",
    "\n",
    "\n",
    "# Function to extract links from a URL------------------------------------------\n",
    "def href_scrapper(url):\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        a_tag = soup.find_all('a')\n",
    "        img_tag = soup.find_all('img')\n",
    "        link_tag = soup.find_all('link')\n",
    "        link_list = list()\n",
    "\n",
    "        for links in a_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in link_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in img_tag:\n",
    "            link_list.append(links.get('src'))\n",
    "\n",
    "        return link_list\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function to create complete links---------------------------------------------\n",
    "def link_former(url):\n",
    "    x = url\n",
    "\n",
    "    if x.startswith(\"http\"):\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        full_url = original_url + x\n",
    "        return full_url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to save links to directory of python file----------------------------\n",
    "def file_save(url, i):\n",
    "    save_error = 0\n",
    "    full_url = url\n",
    "    try:\n",
    "        if str(full_url).endswith(\".htm\"):\n",
    "            name = \"WebPage\" + str(i) + \".htm\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".pdf\"):\n",
    "            name = \"Pdf\" + str(i) + \".pdf\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpg\"):\n",
    "            name = \"Image\" + str(i) + \".jpg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpeg\"):\n",
    "            name = \"Image\" + str(i) + \".jpeg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".png\"):\n",
    "            name = \"Image\" + str(i) + \".png\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".css\"):\n",
    "            name = \"CSS File\" + str(i) + \".css\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        else:\n",
    "            name = \"WebPage\" + str(i) + \".html\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        return m\n",
    "    except:\n",
    "        save_error = save_error + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def crawl(original_url, num_Id, output_file):\n",
    "    \n",
    "    # Accepting input of URL and depth----------------------------------------------\n",
    "    tic = time.time()\n",
    "    \n",
    "    parent_list = [original_url]\n",
    "    url_to_check = get_base_url(original_url)\n",
    "    print(url_to_check)\n",
    "    url_to_check = str(url_to_check)\n",
    "\n",
    "    layer_stop = 1\n",
    "    layer = 0\n",
    "\n",
    "\n",
    "    #initializing all the requiered lists---------------------------------------------\n",
    "    visited_all  = [original_url]\n",
    "    visited_current_layer = []\n",
    "    child_list =[]\n",
    "    child_list_filtered = []\n",
    "    #columns = ['Link','Parent Link', 'Layer']\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Main execution of scrapper----------------------------------------------------\n",
    "\n",
    "    #looping through layers \n",
    "    while layer < layer_stop:\n",
    "    \n",
    "        #looping through URLs in parent-list\n",
    "        for url in parent_list:\n",
    "        \n",
    "            #scraping the children from the parent url----------------------------\n",
    "            if href_scrapper(url) != 0:\n",
    "                child_list = href_scrapper(url)\n",
    "    \n",
    "    \n",
    "            for child in child_list:\n",
    "                if child != None:\n",
    "                    #if child link is of the form \"index.php/blahblah\" and parent ends with '/'\n",
    "                    #---> \"parentlink/index.php/blahblah\"\n",
    "                    if child.startswith('/'):\n",
    "                        child= str(url) + str(child)\n",
    "                    \n",
    "                    if url.endswith('/') and url_to_check not in child:\n",
    "                        child = str(url) + str(child)\n",
    "                    \n",
    "                #normalize the child links-------------------------------------\n",
    "                    child=urltools.normalize(child)  \n",
    "                    \n",
    "\n",
    "                    #filtering out based on 1) External 2) Repeating 3) Invalid links---------------------------\n",
    "                    if url_to_check in child and child not in visited_all and does_page_exist(child)==1:\n",
    "                        child_list_filtered.append(child)\n",
    "                \n",
    "                    #adding everthing to visited all--------------------\n",
    "                    if child not in visited_all:\n",
    "                        child_slash = child + '/'\n",
    "                        visited_all.append(child)  \n",
    "                        visited_all.append(child_slash)\n",
    "                    \n",
    "                    \n",
    "            #adding  the visited and filtered children into the \"current visited layer\" ------ \n",
    "            for child_filtered in child_list_filtered:\n",
    "                visited_current_layer.append(child_filtered)\n",
    "\n",
    "            #creating a Pandas dataframe to store everything for download----------\n",
    "            layer_number = [layer+1]*len(child_list_filtered)\n",
    "            parent_of_child = [url]*len(child_list_filtered)\n",
    "\n",
    "            df_child = pd.DataFrame(child_list_filtered)\n",
    "            df_parent = pd.DataFrame(parent_of_child)\n",
    "            df_layer = pd.DataFrame(layer_number)\n",
    "\n",
    "\n",
    "            df_to_be_added = pd.concat([df_child,df_parent,df_layer], axis=1)\n",
    "            df = pd.concat([df,df_to_be_added],ignore_index=True, axis = 0)\n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "            #emptying the child lists\n",
    "            child_list = []\n",
    "            child_list_filtered = []\n",
    "        \n",
    "        #condition to stop filtering-----------------------------------------------\n",
    "        if not visited_current_layer :\n",
    "            layer_stop = layer_stop \n",
    "        else:\n",
    "            layer_stop += 1\n",
    "\n",
    "    \n",
    "        #child layer is now parent layer--------------------------------------------\n",
    "        parent_list = []\n",
    "    \n",
    "        #we only dont add .png, .jpg , .pdf to the new parent layer \n",
    "        for visited_current in visited_current_layer: \n",
    "            print(visited_current)\n",
    "            if(not visited_current.endswith('.png') and not visited_current.endswith('.jpg') and not  visited_current.endswith('.pdf')):\n",
    "                parent_list.append(visited_current)\n",
    "            \n",
    "            \n",
    "        #displaying the links in different layers----------------------------------\n",
    "        #print(\"Links in LAYER:\" + str(layer+1))\n",
    "        print(\"No of links = \" + str(len(visited_current_layer)))\n",
    "        #print(visited_current_layer)\n",
    "        print(\"\\n\")\n",
    "        visited_current_layer = [] \n",
    "        #updating the layer number\n",
    "        layer +=1\n",
    "    df.to_csv(output_file + '/' + str(num_Id) + '_' + str(url_to_check) +  '.csv', sep=',', encoding='utf-8')\n",
    "    return df, num_Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"https://docs.python.org/3/howto/urllib2.html\")\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "#information based on:\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "p_tag=soup.find_all('p')\n",
    "\n",
    "#extract text from html: \n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "web_text = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import os \n",
    "\n",
    "def scrape(df, scraped_website_path, website_index, website_name):\n",
    "    TempD={}\n",
    "    for index,item in df.iterrows():\n",
    "        try:      \n",
    "            print(item[1])\n",
    "            html = urllib.request.urlopen(str(item[1])).read()\n",
    "            #web_text.append(text_from_html(html))\n",
    "            print(text_from_html(html))\n",
    "            #save_url(str(item[1]), index)\n",
    "            TempD[str(item[1])]=text_from_html(html)\n",
    "        except: \n",
    "            pass\n",
    "    with open(scraped_website_path + '/' + 'json_' + str(website_index) + str(get_base_url(website_name)) +  '.json', 'w') as outfile:  \n",
    "        json.dump(TempD, outfile)\n",
    "        \n",
    "    return 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming conventions \n",
    "\n",
    "#### Naming of csv files with child links,  we name the csv files with the child links :\n",
    "```<index>_<parent website>.csv```\n",
    "\n",
    "#### Naming of JSON files :\n",
    "```<index>_<parent website>.json ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_crawler_all() = Scraper for multiple websites\n",
    "'''\n",
    "Inputs\n",
    "-----------\n",
    "excel_path = Excel file path which contains list of websites to be crawled  \n",
    "sheet = sheet name of this excel file \n",
    "web_col = the column no of in which the website hompages are located \n",
    "index_col = the column with indices of websites\n",
    "parent_col = the column with the parent names\n",
    "output_folder = path of the output folder where we wish to save the websites \n",
    "from_website = the index no. from website (typically starting from 1)\n",
    "to_website = the index no of the website till we want to continue scraping (scraped websites will include the \n",
    "                websites with this index)\n",
    "\n",
    "Output \n",
    "-----------\n",
    "CSV files containing a list of child links for each website are saved into the output_folder path \n",
    "\n",
    "'''\n",
    "def web_crawler_all(excel_path, sheet, web_col, index_col, parent_col,  output_folder, from_website, to_website):\n",
    "    df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "    df = df.iloc[from_website-1 : to_website, :]\n",
    "    print(df)\n",
    "    for index,row in df.iterrows():\n",
    "        num_Id=row[index_col]\n",
    "        original_url=row[web_col]\n",
    "        print(\"Website no : \" + str(num_Id))\n",
    "        print(\"Website name : \" + str(original_url))\n",
    "        \n",
    "        try:\n",
    "            websites= pd.read_csv(output_folder + str(num_Id) + '_' + str(original_url) + '.csv')\n",
    "            print(\"website is already crawled\")\n",
    "        except:\n",
    "            websites, num_Id = crawl(original_url ,num_Id ,output_folder)\n",
    "            \n",
    "        \n",
    "    print(\"All links from the websites indicated have been crawled\")\n",
    "    print(\"Output File :  \" + str(output_folder))\n",
    "    print(\"No. of websites scraped : \" + str(to_website - from_website + 1))\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Serial No.   Company Name                      Website  \\\n",
      "0           1  Mihir Bhatia   https://www.mihirbhatia.com   \n",
      "1           2      Ajin Tom       https://www.ajintom.com   \n",
      "\n",
      "   Has a Website?\\n(1=Yes, 0=No) Static/Dynamic Website?  \\\n",
      "0                              1                       D   \n",
      "1                              1                       D   \n",
      "\n",
      "   If no website- Is present on Facebook?\\n(1=Yes, 0=No)  \n",
      "0                                                NaN      \n",
      "1                                                NaN      \n",
      "Website no : 1\n",
      "Website name : https://www.mihirbhatia.com\n",
      "www.mihirbhatia.com\n",
      "https://www.mihirbhatia.com/s/curriculum-vitae-5.pdf\n",
      "https://www.mihirbhatia.com/bio/\n",
      "https://www.mihirbhatia.com/experience/\n",
      "https://www.mihirbhatia.com/projects/\n",
      "No of links = 4\n",
      "\n",
      "\n",
      "No of links = 0\n",
      "\n",
      "\n",
      "Website no : 2\n",
      "Website name : https://www.ajintom.com\n",
      "www.ajintom.com\n",
      "No of links = 0\n",
      "\n",
      "\n",
      "All links from the websites indicated have been crawled\n",
      "Output File :  C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/crawled_websites\n",
      "No. of websites scraped : 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#executing the crawler function \n",
    "\n",
    "excel_path = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/trial1.xlsx'\n",
    "sheet = 'From List of Companies'\n",
    "web_col = 2\n",
    "index_col = 0\n",
    "parent_col = 1\n",
    "output_folder = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/crawled_websites'\n",
    "from_website = 1 \n",
    "to_website = 2 \n",
    "\n",
    "web_crawler_all(excel_path,sheet, web_col, index_col, parent_col,output_folder, from_website, to_website)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs \n",
    "----------\n",
    "excel_path = Excel file path which contains list of websites to be crawled  \n",
    "sheet = sheet name of this excel file \n",
    "crawled_website_path = path of file where all the crawled websites are stored  \n",
    "scraped_website_path = path of file where all the scraped websites are stored\n",
    "from_website - to_website = website index number from-to which we want to scrape and store in JSON \n",
    "web_col = column number of websites (0,1,2....)\n",
    "\n",
    "Output\n",
    "----------\n",
    "JSON files for each parent website in scraped_website_path folder. The path of the scraped websites \n",
    "is the same as that crawled websites \n",
    "'''\n",
    "\n",
    "def web_scraper_all(excel_path, sheet, crawled_website_path, scraped_website_path, from_website, to_website,web_col):\n",
    "    #get excel sheet containing websites \n",
    "    df = pd.read_excel(excel_path , sheet_name=sheet)\n",
    "    df = df.iloc[from_website-1 : to_website, :]   \n",
    "    \n",
    "    #loop over all the websites and extract child links. Store them in JSON files \n",
    "    for i in range(from_website, to_website+1):\n",
    "        #setting the website index and name \n",
    "        website_index = i \n",
    "        website_name = df.iloc[i-1,web_col]\n",
    "        \n",
    "        #generating filename and getting the csv file of child links \n",
    "        filename = crawled_website_path + '/'+ str(website_index)+ '_' + str(get_base_url(website_name)) + \".csv\"\n",
    "        df_of_child_links = pd.read_csv(filename)\n",
    "        \n",
    "        #executing the scrape function \n",
    "        flag = scrape(df_of_child_links , scraped_website_path, website_index, website_name) \n",
    "        \n",
    "        #checkpoint to see if scraping is complete \n",
    "        if flag == 1 :\n",
    "            print(website_name + ' has been scraped successfully')\n",
    "            print('scraping percentage complete = ' + str((i/to_website)*100) + '%')\n",
    "            flag = 0 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mihirbhatia.com/s/curriculum-vitae-5.pdf\n",
      "https://www.mihirbhatia.com/bio/\n",
      "https://www.mihirbhatia.com/experience/\n",
      "https://www.mihirbhatia.com/projects/\n",
      "            RESUME  Bio  HOME  EXPERIENCE  PROJECTS      RESUME  Bio  HOME  EXPERIENCE  PROJECTS                                                         ASPIRING DATA SCIENTIST | RESEARCHER | MECHANICAL ENGINEER          Select project category   DATA SCIENCE    INFORMATION ENGINEERING    MACHINE VISION    ROBOTICS  DATA SCIENCE Some of the instances that I've played around with data: High Energy Physics particle tracking in CERN detectors (Kaggle) - Ongoing Stock Portfolio Performance by Weighted Stock Selection Prediction of Electricity used for space conditioning for Buildings in the Pacific Region Prediction of Electricity costs (using Regression, Random Forests and MARS)  High Energy Physics particle tracking in CERN detectors (Kaggle)         A Kaggle competition seemed like the appropriate challenge for the Summer. We've formed a team of 4 graduates, to work on this amazing particle tracking project from CERN. They've provided us, with a a massive 70+ GB training set of simulations from the Large Hadron Collider at CERN. This is an amazing opportunity to test our skills and play around with a massive. Physicists at CERN need to make sense of PETABYTES of data during their experiments where they smash protons travelling at near light speed to simulate mini-big bangs on earth. It's awesome that as students we can contribute to us getting closer to uncovering the origins of the universe. We'll upload our code on Github and keep our progress updated in our linked report below   GITHUB   Stock Portfolio Performance by Weighted Stock Selection         The first ideas that came to my mind as a student in my Predictive Modelling class, was that I could use the techniques taught in the class, to make predictions about stocks and make money. It seemed like an easy way to the riches. But as I started reading into the it, there were so many factors that affected stock markets. Trade volume, annual reports, product launches, mergers and acquisitions as well as people's perceptions of how the company would perform (sentiment analysis) was a big part of predicting where the markets would go and I'm still a long way from building bots that would actually be competitive, given the considerations of the number of factors actually influencing the markets and the fact that the data is Time Series made it all the more complex. As I was looking through some data-sets from the UCI machine learning repository (which by the way is a great resource for datasets, they also classify their problems into classifcation, regression, multivariate etc.), I found this dataset that laid down the Stock Portfolio Performances in terms of returns, risk and win-rates (OUTPUTs) depending various factors (INPUTs) used in a \"scoring system\" to select stocks from the S&P500 index. The outputs were simulated I felt like this would be an exciting starting point for one of my first full fledged predictive analytics projects. Check out the github link and project report for more details   PROJECT REPORT    GITHUB LINK  Building Electricity Usage Prediction In this project, I used the CBECs Dataset - which has data about buildings from various regions, and they've collected data about anything and everything in the buildings that were surveyed. This data can be used for the prediction of Electricity consumption using just a few relevant parameters about the building. Using my algorithm I was able to bring down the number of variables to required to build a good predictive model to approximately 10 dependent variables using a Random Forests model. It was awesome that I could predict to a great extent how much electricity a building in the Pacific Region would consume by using this model. Check out the github link and project report for more details.   PROJECT  REPORT    GITHUB LINK  Electricity cost prediction for buildings This was an elementary level project to practice the use of Linear Regression, CART, Random Forests and MARS on a data-set provided in-class. This data-set was relatively clean. The aim of this project was to work on understanding and implementing the above mentioned modeling methods.   GITHUB   Back to Top INFORMATION ENGINEERING Web Crawler I built a simple web crawler that is capable of taking in parent website as input and outputs a list of web pages that it crawls from that particular website. This web crawler will be used in the WHIN - Supply Chain Project that I have undertaken for the summer.   CRAWLER GITHUB  Online To-Do List I made an online to do list as part of my information engineering class. This mini-project was meant for us to get a grasp on HTML, CSS and Javascript. It's a color coded online to-do list. Feel free to download and use it.         Click image to zoom       TO DO LIST    GITHUB  Drug Interaction System         I used the RESTful API provided on the NIH website that contains information about a large number of drugs. I  developed an app that used PhP to parse through the descriptions of the drugs to check if there were interactions between the drugs. Such an app could be could be integrated into hospital IT systems, where doctors enter prescription drugs. The output could be warning letting the doctor know that there are interactions between the drugs .   GITHUB    Back to Top MACHINE VISION  Background Extraction The aim of this mini-project was the extract a background image from this room where there were student continuously walking around. This can be useful for robots to differentiate between background and foreground and develop a MAP of the region that it's trying to explore even in the presence of moving entities in the foreground.                      The Mixture of Gaussian (MoG) method helps us to distinguish between foreground and background and then we can extract individual pixel values from the different time periods of the video to reconstruct an image of the background.Although, this method is not perfect as is visible from the black patches in the result.   GITHUB  A Glimpse of the Past A view into the past obtained by stitching an old image of a building onto a new one                     Result after stitching past image into present image         For the code click on the GitHub button below and use the Python code to try it out for yourself.   GITHUB  Stitching Images Stitching Images using SIFT key points and descriptors & a Homography matrix. This was a rather simple exercise and can be scaled up for more images to generate a panorama                               GITHUB   ROBOTICS           ABU ROBOCON is an international level robotics competition in the Asia Pacific Region for undergraduate students who are passionate about robotics and problem solving. Each year, the committee sets an interesting theme and a challenging problem statement associated with that theme. The student of the competition design and manufacturing robots from scratch. As a student of K.J. Somaiya College of Engineering and a part of Team ROBOCON KJSCE , I was a part of 3 awesome projects. I was the team leader of the team during ROBOCON 2016. Not only did this experience give me an insight into the intricacies of robotics/automation but it also taught me how to handle complex projects with multiple modules and working under pressure with tight deadlines. FRISBEE ROBOT - ABU ROBOCON 2017  BADMINTON ROBOTS - ABU ROBOCON 2015                   Back to Top                                              Powered by Squarespace         \n",
      "https://www.mihirbhatia.com has been scraped successfully\n",
      "scraping percentage complete = 50.0%\n",
      "https://www.ajintom.com has been scraped successfully\n",
      "scraping percentage complete = 100.0%\n"
     ]
    }
   ],
   "source": [
    "#executing the scraper function \n",
    "\n",
    "excel_path = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/trial1.xlsx'\n",
    "sheet = 'From List of Companies'\n",
    "crawled_website_path = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/crawled_websites'\n",
    "scraped_website_path = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/scraped_websites'\n",
    "from_website = 1\n",
    "to_website = 1 \n",
    "to_website = 2\n",
    "web_col = 2 \n",
    "\n",
    "web_scraper_all(excel_path, sheet, crawled_website_path, scraped_website_path, from_website, to_website,web_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('json1.json') as f1:\n",
    "    data1 = json.load(f1)\n",
    "    \n",
    "with open('json2.json') as f2:\n",
    "    data2 = json.load(f2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.ajintom.com\n",
      "https://www.ajintom.com/\n",
      "https://www.ajintom.com/bio/\n",
      "https://www.ajintom.com/music/\n",
      "https://www.ajintom.com/tech/\n",
      "https://www.ajintom.com/s/Ajin-Tom-Resume18.pdf\n",
      "https://www.ajintom.com/contact/\n",
      "No of links = 6\n",
      "\n",
      "\n",
      "No of links = 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_url = 'https://www.ajintom.com'\n",
    "num_Id  = 1 \n",
    "output_folder = 'C:/Users/mihirbhatia999/Desktop/DCMME/scraped_data/crawled_websites'\n",
    "df, num_Id = crawl(original_url, num_Id, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
