{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract links from a URL------------------------------------------\n",
    "def href_scrapper(url):\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        a_tag = soup.find_all('a')\n",
    "        img_tag = soup.find_all('img')\n",
    "        link_tag = soup.find_all('link')\n",
    "        link_list = list()\n",
    "\n",
    "        for links in a_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in link_tag:\n",
    "            link_list.append(links.get('href'))\n",
    "\n",
    "        for links in img_tag:\n",
    "            link_list.append(links.get('src'))\n",
    "\n",
    "        return link_list\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create complete links---------------------------------------------\n",
    "def link_former(url):\n",
    "    x = url\n",
    "\n",
    "    if x.startswith(\"http\"):\n",
    "        return x\n",
    "\n",
    "    else:\n",
    "        full_url = original_url + x\n",
    "        return full_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save links to directory of python file----------------------------\n",
    "def file_save(url, i):\n",
    "    save_error = 0\n",
    "    full_url = url\n",
    "    try:\n",
    "        if str(full_url).endswith(\".htm\"):\n",
    "            name = \"WebPage\" + str(i) + \".htm\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".pdf\"):\n",
    "            name = \"Pdf\" + str(i) + \".pdf\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpg\"):\n",
    "            name = \"Image\" + str(i) + \".jpg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".jpeg\"):\n",
    "            name = \"Image\" + str(i) + \".jpeg\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".png\"):\n",
    "            name = \"Image\" + str(i) + \".png\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        elif str(full_url).endswith(\".css\"):\n",
    "            name = \"CSS File\" + str(i) + \".css\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        else:\n",
    "            name = \"WebPage\" + str(i) + \".html\"\n",
    "            urllib.request.urlretrieve(full_url, name)\n",
    "            m = i + 1\n",
    "\n",
    "        return m\n",
    "    except:\n",
    "        save_error = save_error + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepting input of URL and depth----------------------------------------------\n",
    "\n",
    "original_url = \"https://www.dayton-phoenix.com/\"\n",
    "parent_list = [original_url]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links in LAYER:1\n",
      "No of links = 28\n",
      "['https://www.dayton-phoenix.com/sitemap.php', 'https://www.dayton-phoenix.com/sitecredits.php', 'https://www.dayton-phoenix.com/divisions.php', 'https://www.dayton-phoenix.com/news.php', 'https://www.dayton-phoenix.com/productcategories.php', 'https://www.dayton-phoenix.com/servicecenters.php', 'https://www.dayton-phoenix.com/contact.php', 'https://www.dayton-phoenix.com//divisions.php', 'https://www.dayton-phoenix.com/index.php?dt=FL', 'https://www.dayton-phoenix.com/index.php?dt=PL', 'https://www.dayton-phoenix.com/index.php?dt=OH', 'https://www.dayton-phoenix.com/index.php?dt=ES', 'https://www.dayton-phoenix.com/index.php?dt=SV', 'https://www.dayton-phoenix.com/news.php?newsId=38', 'https://www.dayton-phoenix.com/news.php?newsId=37', 'https://www.dayton-phoenix.com/news.php?newsId=36', 'https://www.dayton-phoenix.com/news.php?newsId=35', 'https://www.dayton-phoenix.com/productcategorydetail.php?categoryId=81', 'https://www.dayton-phoenix.com/productcategories.php?categoryId=60', 'https://www.dayton-phoenix.com/dpg.css', 'https://www.dayton-phoenix.com/colorbox.css', 'https://www.dayton-phoenix.com/rss.php', 'https://www.dayton-phoenix.com/i/dpgLogo.gif', 'https://www.dayton-phoenix.com/i/divFL.png', 'https://www.dayton-phoenix.com/i/divButton-on.gif', 'https://www.dayton-phoenix.com/i/divButton-off.gif', 'https://www.dayton-phoenix.com/i/catThumb/AuxGenerator.png', 'https://www.dayton-phoenix.com/i/catThumb/FuelPumpMotor.png']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links in LAYER:2\n",
      "No of links = 5\n",
      "['mailto:mdowling@dayton-phoenix.com', 'https://www.dayton-phoenix.com/news.php/documents/uploads/DPG Announces new ADVANTAGE SERVICES for GP 38-40 and SD40 Locomotives .pdf', 'https://www.dayton-phoenix.com/news.php?newsId=37/documents/uploads/DPG Announces new SERIES Rooftop AC Phase out of older unit May 4  2018 R .pdf', 'https://www.dayton-phoenix.com/news.php?newsId=36/documents/uploads/Press Release DPG Announces Attendance at ASLRRA Connections April  2018.pdf', 'https://www.dayton-phoenix.com/rss.php/documents/uploads/Press Release DPG Announces Attendance at Railwa 2017 Interchange Conference.pdf']\n",
      "\n",
      "\n",
      "Links in LAYER:3\n",
      "No of links = 0\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main execution of scrapper----------------------------------------------------\n",
    "\n",
    "#initializing all the rquiered lists\n",
    "visited_all  = [original_url]\n",
    "visited_current_layer = []\n",
    "child_list =[]\n",
    "child_list_filtered = []\n",
    "temp=[]\n",
    "#columns = ['Link','Parent Link', 'Layer']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "layer_stop = 1\n",
    "layer = 0\n",
    "\n",
    "#looping through layers \n",
    "while layer < layer_stop:\n",
    "    \n",
    "    #looping through URLs in parent-list\n",
    "    for url in parent_list:\n",
    "        \n",
    "        #scraping the children from the parent url----------------------------\n",
    "        if href_scrapper(url) != 0:\n",
    "            child_list = href_scrapper(url)\n",
    "        \n",
    "        #filtering out external and repeating links---------------------------\n",
    "        for child in child_list:\n",
    "            if child != None:\n",
    "                child=urltools.normalize(child)\n",
    "                if child not in temp:\n",
    "                    temp.append(child)\n",
    "                    if child.startswith('/'):\n",
    "                        child= str(url) + str(child)\n",
    "                \n",
    "                      # if the href is apage.php, and parent ends with a '/' then it needs the parent url added to href\n",
    "                \n",
    "                    if \"dayton-phoenix.com\" not in child:\n",
    "                        if url.endswith('/'):\n",
    "                            child= str(parent_list[0]) + str(child)\n",
    "                child=urltools.normalize(child)\n",
    "                \n",
    "                if \"dayton-phoenix.com\" in child and child not in visited_all:\n",
    "                    child_list_filtered.append(child)\n",
    "                    visited_all.append(child)\n",
    "        \n",
    "        for child_filtered in child_list_filtered:\n",
    "            visited_current_layer.append(child_filtered)\n",
    "\n",
    "        #creating a Pandas dataframe to store everything for download----------\n",
    "        layer_number = [layer+1]*len(child_list_filtered)\n",
    "        parent_of_child = [url]*len(child_list_filtered)\n",
    "\n",
    "        df_child = pd.DataFrame(child_list_filtered)\n",
    "        df_parent = pd.DataFrame(parent_of_child)\n",
    "        df_layer = pd.DataFrame(layer_number)\n",
    "\n",
    "\n",
    "        df_to_be_added = pd.concat([df_child,df_parent,df_layer], axis=1)\n",
    "        df = pd.concat([df,df_to_be_added],ignore_index=True, axis = 0)\n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        #emptying the child lists\n",
    "        child_list = []\n",
    "        child_list_filtered = []\n",
    "        \n",
    "    #condition to stop filtering-----------------------------------------------\n",
    "    if not visited_current_layer :\n",
    "        layer_stop = layer_stop \n",
    "    else:\n",
    "        layer_stop += 1\n",
    "\n",
    "    \n",
    "    #child layer is now parent layer--------------------------------------------\n",
    "    parent_list = visited_current_layer\n",
    "    \n",
    "    #displaying the links in different layers----------------------------------\n",
    "    print(\"Links in LAYER:\" + str(layer+1))\n",
    "    print(\"No of links = \" + str(len(visited_current_layer)))\n",
    "    print(visited_current_layer)\n",
    "    print(\"\\n\")\n",
    "    visited_current_layer = [] \n",
    "    #updating the layer number\n",
    "    layer +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(temp))\n",
    "print(len(visited_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    " \n",
    "df = pd.read_excel('Downloads\\Final Final List of Companies.xlsx', sheetname='Contacted Companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://www.dayton-phoenix.com//divisions.php\n",
       "1    https://www.dayton-phoenix.com//divisions.php/\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "#import urllib.request\n",
    "import urllib\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
